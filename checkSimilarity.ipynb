# -*- coding: utf-8 -*-
"""
Created on Fri Dec  7 4:30 2018

@author: Andre Holder

Description: BMI 500 Lab 15:Checking code similarity (using deid2 files)

Universal Sentence Encoder code adapted from tensor flow website: 
https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=Q8F4LNGFqOiq

"""

import os
import pandas as pd # Python Data Analysis Library
import numpy as np
import matplotlib.pyplot as plot
import Levenshtein as L
import re
import seaborn as sns
import tensorflow as tf
import tensorflow_hub as hub

### Open the 8 deid codes (one for each student, converted to txt files) and import into Python environment as a string
# Each string is given a name with a number at the end (e.g. deid2)
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid1.txt', 'r') as myfile:
    deid1=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid2.txt', 'r') as myfile:
    deid2=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid3.txt', 'r') as myfile:
    deid3=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid4.txt', 'r') as myfile:
    deid4=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid5.txt', 'r') as myfile:
    deid5=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid6.txt', 'r') as myfile:
    deid6=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid7.txt', 'r') as myfile:
    deid7=myfile.read().replace('\n', '\t')
with open('C:\\Users\\d-r\\Dropbox\\EMORY\\RESEARCH\\KL2_EXECUTION\\Classes\\BMI500\\Submitted_HWs\\HW15-Code_similarity\\deid8.txt', 'r') as myfile:
    deid8=myfile.read().replace('\n', '\t')
    
### Testing string similarity metrics
print('Two simple similarity metrics were tested (Jaro and Absolute Levenshtein distance) against 8 different codes.\n')

# Jaro string similarity metric
print('The Jaro similarity metric measures are:')
print('code 1 vs code 2: ', L.jaro(deid1,deid2))  
print('code 1 vs code 3: ', L.jaro(deid1,deid3)) 
print('code 1 vs code 4: ', L.jaro(deid1,deid4))
print('code 1 vs code 5: ', L.jaro(deid1,deid5))
print('code 1 vs code 6: ', L.jaro(deid1,deid6))
print('code 1 vs code 7: ', L.jaro(deid1,deid7))

print('code 2 vs code 3: ', L.jaro(deid2,deid3))
print('code 2 vs code 4: ', L.jaro(deid2,deid4))
print('code 2 vs code 5: ', L.jaro(deid2,deid5))
print('code 2 vs code 6: ', L.jaro(deid2,deid6))
print('code 2 vs code 7: ', L.jaro(deid2,deid7))
print('code 2 vs code 8: ', L.jaro(deid2,deid8))

print('code 3 vs code 4: ', L.jaro(deid3,deid4)) 
print('code 3 vs code 5: ', L.jaro(deid3,deid5))
print('code 3 vs code 6: ', L.jaro(deid3,deid6))
print('code 3 vs code 7: ', L.jaro(deid3,deid7))
print('code 3 vs code 8: ', L.jaro(deid3,deid8))

print('code 4 vs code 5: ', L.jaro(deid4,deid5))
print('code 4 vs code 6: ', L.jaro(deid4,deid6))
print('code 4 vs code 7: ', L.jaro(deid4,deid7))
print('code 4 vs code 8: ', L.jaro(deid4,deid8))

print('code 5 vs code 6: ', L.jaro(deid5,deid6))
print('code 5 vs code 7: ', L.jaro(deid5,deid7))
print('code 5 vs code 8: ', L.jaro(deid5,deid8))

print('code 6 vs code 7: ', L.jaro(deid6,deid7))
print('code 6 vs code 8: ', L.jaro(deid6,deid8))

print('code 7 vs code 8: ', L.jaro(deid7,deid8), '\n')


# Absolute Levenshtein distance
print('The Absolute Levenshtein distance metric measures are:')
print('code 1 vs code 2: ', L.distance(deid1,deid2))  
print('code 1 vs code 3: ', L.distance(deid1,deid3)) 
print('code 1 vs code 4: ', L.distance(deid1,deid4))
print('code 1 vs code 5: ', L.distance(deid1,deid5))
print('code 1 vs code 6: ', L.distance(deid1,deid6))
print('code 1 vs code 7: ', L.distance(deid1,deid7))

print('code 2 vs code 3: ', L.distance(deid2,deid3))
print('code 2 vs code 4: ', L.distance(deid2,deid4))
print('code 2 vs code 5: ', L.distance(deid2,deid5))
print('code 2 vs code 6: ', L.distance(deid2,deid6))
print('code 2 vs code 7: ', L.distance(deid2,deid7))
print('code 2 vs code 8: ', L.distance(deid2,deid8))

print('code 3 vs code 4: ', L.distance(deid3,deid4)) 
print('code 3 vs code 5: ', L.distance(deid3,deid5))
print('code 3 vs code 6: ', L.distance(deid3,deid6))
print('code 3 vs code 7: ', L.distance(deid3,deid7))
print('code 3 vs code 8: ', L.distance(deid3,deid8))

print('code 4 vs code 5: ', L.distance(deid4,deid5))
print('code 4 vs code 6: ', L.distance(deid4,deid6))
print('code 4 vs code 7: ', L.distance(deid4,deid7))
print('code 4 vs code 8: ', L.distance(deid4,deid8))

print('code 5 vs code 6: ', L.distance(deid5,deid6))
print('code 5 vs code 7: ', L.distance(deid5,deid7))
print('code 5 vs code 8: ', L.distance(deid5,deid8))

print('code 6 vs code 7: ', L.distance(deid6,deid7))
print('code 6 vs code 8: ', L.distance(deid6,deid8))

print('code 7 vs code 8: ', L.distance(deid7,deid8))

module_url = "https://tfhub.dev/google/universal-sentence-encoder/2" #@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3"]

# Import the Universal Sentence Encoder's TF Hub module
embed = hub.Module(module_url)

# Compute a representation for each message, showing various lengths supported.
word = "Elephant"
sentence = "I am a sentence for which I would like to get its embedding."
paragraph = (
    "Universal Sentence Encoder embeddings also support short paragraphs. "
    "There is no hard limit on how long the paragraph is. Roughly, the longer "
    "the more 'diluted' the embedding will be.")
messages = [word, sentence, paragraph]

# Reduce logging output.
tf.logging.set_verbosity(tf.logging.ERROR)

with tf.Session() as session:
  session.run([tf.global_variables_initializer(), tf.tables_initializer()])
  message_embeddings = session.run(embed(messages))

  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
    print("Message: {}".format(messages[i]))
    print("Embedding size: {}".format(len(message_embedding)))
    message_embedding_snippet = ", ".join(
        (str(x) for x in message_embedding[:3]))
    print("Embedding: [{}, ...]\n".format(message_embedding_snippet))

# Semantic Textual Similarity functions
def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")


def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):
  message_embeddings_ = session_.run(
      encoding_tensor, feed_dict={input_tensor_: messages_})
  plot_similarity(messages_, message_embeddings_, 90)
  
# Create common phrases found in the code that could group similar tasks
messages = [
    # Describe the identifier
    "date_pattern",
    "phone_pattern",
    "age_pattern",

    # Name a compiler
    "age_reg",
    "date_reg",
    "ph_reg",

    # for loop to find the iteration
    "for match in",

    # Create the str to write to a file
    "result = str(str(start_loc) + ' ' + str(start_loc) +' '+ str(end_loc))'",
    "result = str(match.span(1)[0] - offset) + ' ' + str(match.span(1)[1] - offset) + ' ' + str(match.span(1)[1] - offset)",
    "result = str(match.start()+1-offset) + ' ' + str(match.start()+1-offset) +' '+ str(match.end()-1-offset)",
    "result = str(match.start() - offset) + ' ' + str(match.start() - offset) + ' ' + str(match.end() - offset)",
    
    # Write it
    "output_handle.write",
    
    # Open output file 
    "with open as output_file",
    
    #Extract
    "re.findall("
]

similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))
similarity_message_encodings = embed(similarity_input_placeholder)
with tf.Session() as session:
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  run_and_plot(session, similarity_input_placeholder, messages,
               similarity_message_encodings)
"""
L.Hamming(str1,str2)  # Hamming distance of two strings 
L.jaro_winkler(str1,str2) # Jaro-Winkler string similarity
L.ratio(str1,str2)    # Similarity of two strings
"""


### TESTING SIMILARITY USING THE UNIVERSAL SENTENCE ENCODER
import seaborn as sns
import tensorflow as tf
import tensorflow_hub as hub
module_url = "https://tfhub.dev/google/universal-sentence-encoder/2" #@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3"]

# Import the Universal Sentence Encoder's TF Hub module
embed = hub.Module(module_url)

# Reduce logging output.
tf.logging.set_verbosity(tf.logging.ERROR)

# Semantic Textual Similarity functions
def plot_similarity(labels, features, rotation):
  corr = np.inner(features, features)
  sns.set(font_scale=1.2)
  g = sns.heatmap(
      corr,
      xticklabels=labels,
      yticklabels=labels,
      vmin=0,
      vmax=1,
      cmap="YlOrRd")
  g.set_xticklabels(labels, rotation=rotation)
  g.set_title("Semantic Textual Similarity")


def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):
  message_embeddings_ = session_.run(
      encoding_tensor, feed_dict={input_tensor_: messages_})
  plot_similarity(messages_, message_embeddings_, 90)
  
# Create common phrases found in the code that could group similar tasks
messages = [
    # Describe the identifier
    "date_pattern",
    "phone_pattern",
    "age_pattern",

    # Name a compiler
    "age_reg",
    "date_reg",
    "ph_reg",

    # for loop to find the iteration
    "for match in",

    # Create the str to write to a file
    "result = str(str(start_loc) + ' ' + str(start_loc) +' '+ str(end_loc))'",
    "result = str(match.span(1)[0] - offset) + ' ' + str(match.span(1)[1] - offset) + ' ' + str(match.span(1)[1] - offset)",
    "result = str(match.start()+1-offset) + ' ' + str(match.start()+1-offset) +' '+ str(match.end()-1-offset)",
    "result = str(match.start() - offset) + ' ' + str(match.start() - offset) + ' ' + str(match.end() - offset)",
    
    # Write it
    "output_handle.write",
    
    # Open output file 
    "with open as output_file",
    
    #Extract
    "re.findall("
]

similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))
similarity_message_encodings = embed(similarity_input_placeholder)
with tf.Session() as session:
  session.run(tf.global_variables_initializer())
  session.run(tf.tables_initializer())
  run_and_plot(session, similarity_input_placeholder, messages,
               similarity_message_encodings)
